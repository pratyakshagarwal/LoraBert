{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO>:\n",
    "- get data from kaggle try some processing techniques then tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; \n",
    "import torch\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = \"..\\data\\FinancialPhraseBank-v1.0.zip\"\n",
    "extract_dir = \"../data/\"\n",
    "base_path = '../data/FinancialPhraseBank-v1.0'\n",
    "corpus_path = \"../data/news_corpus.txt\"\n",
    "tokenizer_path = \"../models/finbpe_tokenizer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zipfile(zip_file_path, extract_dir):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_content = zip_ref.namelist()\n",
    "        print(f\"File inside the zip file: {zip_content}\")\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"File extracted at path: {extract_dir}\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "def handle_data(file_paths: list, to_save: bool = True):\n",
    "    content = {'news': [], 'sentiment': []}   \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Handling file: {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:data = file.read()\n",
    "            data_mid = [h.split(\"@\") for h in data.split('\\n') if '@' in h]\n",
    "            for news, sentiment in data_mid:\n",
    "                content['news'].append(news);content['sentiment'].append(sentiment)\n",
    "            print(f\"{file_path} handled successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    if to_save:\n",
    "        to_return = pd.DataFrame(content)\n",
    "        to_return.to_csv(to_save)\n",
    "        print(f\"Data saved as {to_save}\")\n",
    "    else:\n",
    "        to_return = content\n",
    "        print(\"Data processed, not saved as DataFrame\")\n",
    "    return to_return\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "def reencode_file_to_utf8(filepath: str):\n",
    "    with open(filepath, 'r', encoding='ISO-8859-1') as f:content = f.read()\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:f.write(content)\n",
    "    print(f\"File re-encoded to UTF-8 and saved at: {filepath}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "def save_corpus(data: pd.DataFrame, filepath: str) -> None:\n",
    "    # Ensure the directory exists\n",
    "    directory = os.path.dirname(filepath)\n",
    "    if not os.path.exists(directory):os.makedirs(directory)\n",
    "    text = ' '.join(data['news'].tolist())\n",
    "    with open(filepath, 'w') as cnt:cnt.write(text)\n",
    "    print(f\"News Corpus saved at: {filepath}\")\n",
    "    reencode_file_to_utf8(filepath)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def get_tokenizer(train:bool=True, data_path:list=None, vocab_size:int=None, tokenizer_path:str=\"models/finbpe_tokenizer.json\", special_tokens:list=None):\n",
    "    if train:\n",
    "        special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"] if special_tokens is None else special_tokens\n",
    "        tokenizer = Tokenizer(BPE()); tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(vocab_size=vocab_size, min_frequency=2, special_tokens=special_tokens)\n",
    "        tokenizer.train(data_path); tokenizer.save(tokenizer_path)\n",
    "        print(f\"Tokenizer saved at: {tokenizer_path}\")\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(path=tokenizer_path)\n",
    "        print(f\"Tokenizer loaded from {tokenizer_path}\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File inside the zip file: ['FinancialPhraseBank-v1.0/', 'FinancialPhraseBank-v1.0/License.txt', '__MACOSX/', '__MACOSX/FinancialPhraseBank-v1.0/', '__MACOSX/FinancialPhraseBank-v1.0/._License.txt', 'FinancialPhraseBank-v1.0/README.txt', '__MACOSX/FinancialPhraseBank-v1.0/._README.txt', 'FinancialPhraseBank-v1.0/Sentences_50Agree.txt', 'FinancialPhraseBank-v1.0/Sentences_66Agree.txt', 'FinancialPhraseBank-v1.0/Sentences_75Agree.txt', 'FinancialPhraseBank-v1.0/Sentences_AllAgree.txt']\n",
      "File extracted at path: ../data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data/FinancialPhraseBank-v1.0\\\\Sentences_50Agree.txt',\n",
       " '../data/FinancialPhraseBank-v1.0\\\\Sentences_66Agree.txt',\n",
       " '../data/FinancialPhraseBank-v1.0\\\\Sentences_75Agree.txt',\n",
       " '../data/FinancialPhraseBank-v1.0\\\\Sentences_AllAgree.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_zipfile(zip_file_path, extract_dir)\n",
    "file_paths = os.listdir(extract_dir + 'FinancialPhraseBank-v1.0')[2:]\n",
    "full_paths = [os.path.join(base_path, file) for file in file_paths]; full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling file: ../data/FinancialPhraseBank-v1.0\\Sentences_50Agree.txt\n",
      "../data/FinancialPhraseBank-v1.0\\Sentences_50Agree.txt handled successfully\n",
      "Handling file: ../data/FinancialPhraseBank-v1.0\\Sentences_66Agree.txt\n",
      "../data/FinancialPhraseBank-v1.0\\Sentences_66Agree.txt handled successfully\n",
      "Handling file: ../data/FinancialPhraseBank-v1.0\\Sentences_75Agree.txt\n",
      "../data/FinancialPhraseBank-v1.0\\Sentences_75Agree.txt handled successfully\n",
      "Handling file: ../data/FinancialPhraseBank-v1.0\\Sentences_AllAgree.txt\n",
      "../data/FinancialPhraseBank-v1.0\\Sentences_AllAgree.txt handled successfully\n",
      "Data saved as ../data/finnews.csv\n"
     ]
    }
   ],
   "source": [
    "data = handle_data(file_paths=full_paths, to_save=extract_dir+\"finnews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Corpus saved at: ../data/news_corpus.txt\n",
      "File re-encoded to UTF-8 and saved at: ../data/news_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "save_corpus(data, corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved at: ../models/finbpe_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(train=True, data_path=[corpus_path], vocab_size=2500, tokenizer_path=tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     8951\n",
       "positive    3988\n",
       "negative    1841\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
